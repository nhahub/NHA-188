# -*- coding: utf-8 -*-
"""click.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1r6lV8UYB1HKSvZS8BS8CRrVXs8gHNCmi
"""

!ls /content | grep spark

!wget -O /content/spark.tgz https://archive.apache.org/dist/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz
!tar -xzf /content/spark.tgz -C /content

!ls /content/spark-3.5.0-bin-hadoop3/bin | grep spark-submit

import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-11-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.5.0-bin-hadoop3"
os.environ["PATH"] += os.pathsep + os.path.join(os.environ["SPARK_HOME"], "bin")

from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("Churn Prediction") \
    .config("spark.jars", "/content/gcs-connector.jar") \
    .config("spark.hadoop.fs.gs.impl", "com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem") \
    .config("spark.hadoop.google.cloud.auth.service.account.enable", "true") \
    .config("spark.hadoop.google.cloud.auth.service.account.json.keyfile", "/content/key.json") \
    .getOrCreate()

spark

from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("Churn Prediction") \
    .config("spark.jars", "/content/gcs-connector.jar") \
    .config("spark.hadoop.fs.gs.impl", "com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem") \
    .config("spark.hadoop.google.cloud.auth.service.account.enable", "true") \
    .config("spark.hadoop.google.cloud.auth.service.account.json.keyfile", "/content/key.json") \
    .getOrCreate()

df = spark.read.parquet("gs://bigdata-ai-datalake/enriched/clickstream_enriched.parquet")
df.show(5)

numeric_cols = ["age", "tenure_months", "monthly_charges", "total_charges", "avg_sentiment", "conversion_rate"]
categorical_cols = ["contract_type", "internet_service", "payment_method"]
label_col = "churn_flag"  # ده العمود الهدف

from pyspark.sql.functions import when, col

df = df.withColumn("label", when(col("returns") > 0, 1).otherwise(0))

numeric_cols = ["session_duration_seconds", "views", "purchases", "total_sales_amount", "conversion_rate", "return_rate", "total_events"]
categorical_cols = ["country", "browser", "payment_method", "segment"]

from pyspark.ml.feature import StringIndexer

indexers = [StringIndexer(inputCol=c, outputCol=c+"_idx", handleInvalid="keep") for c in categorical_cols]

from pyspark.ml.feature import OneHotEncoder

encoders = [OneHotEncoder(inputCol=c+"_idx", outputCol=c+"_vec", handleInvalid="keep") for c in categorical_cols]

from pyspark.ml.feature import VectorAssembler

assembler_inputs = numeric_cols + [c+"_vec" for c in categorical_cols]
assembler = VectorAssembler(inputCols=assembler_inputs, outputCol="features", handleInvalid="keep")

from pyspark.ml import Pipeline

pipeline_stages = indexers + encoders + [assembler]
pipeline = Pipeline(stages=pipeline_stages)

pipeline_model = pipeline.fit(df)
df_transformed = pipeline_model.transform(df).select("features", "label")

train_df, test_df = df_transformed.randomSplit([0.8, 0.2], seed=42)
print("Train/test counts:", train_df.count(), test_df.count())

from pyspark.ml.classification import RandomForestClassifier

# إعداد نموذج RandomForest
rf = RandomForestClassifier(labelCol="label", featuresCol="features", numTrees=100, maxDepth=6, seed=42)

# تدريب النموذج على مجموعة التدريب
rf_model = rf.fit(train_df)

predictions = rf_model.transform(test_df)

from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator

# Accuracy, F1, Precision, Recall
e_acc = MulticlassClassificationEvaluator(labelCol="label", predictionCol="prediction", metricName="accuracy")
e_f1 = MulticlassClassificationEvaluator(labelCol="label", predictionCol="prediction", metricName="f1")
e_prec = MulticlassClassificationEvaluator(labelCol="label", predictionCol="prediction", metricName="weightedPrecision")
e_rec = MulticlassClassificationEvaluator(labelCol="label", predictionCol="prediction", metricName="weightedRecall")

accuracy = e_acc.evaluate(predictions)
f1 = e_f1.evaluate(predictions)
precision = e_prec.evaluate(predictions)
recall = e_rec.evaluate(predictions)

# ROC-AUC
e_auc = BinaryClassificationEvaluator(labelCol="label", rawPredictionCol="rawPrediction", metricName="areaUnderROC")
roc_auc = e_auc.evaluate(predictions)

print(f"Accuracy: {accuracy:.4f}")
print(f"F1 score: {f1:.4f}")
print(f"Precision (weighted): {precision:.4f}")
print(f"Recall (weighted): {recall:.4f}")
print(f"ROC-AUC: {roc_auc:.4f}")

preds_and_labels = predictions.select("prediction", "label")
cm = preds_and_labels.groupBy("label", "prediction").count().orderBy("label", "prediction")
print("Confusion matrix (label, prediction, count):")
cm.show(truncate=False)

cloud_model_path = "gs://bigdata-ai-datalake/enriched/clickstream_enriched.parquet"
!rm -rf /content/models/churn_model_spark
rf_model.save(cloud_model_path)
print("Model saved locally to", cloud_model_path)

new_data = spark.createDataFrame([
    (0, 5, 1, 100.0, 0.2, 0.1, 7, "US", "Android App", "Cash", "Buyer"),
    (0, 2, 0, 50.0, 0.0, 0.0, 3, "IT", "IOS App", "Credit Card", "Browser")
], schema=numeric_cols + categorical_cols)

# أضيفي label مؤقت لو محتاج (مش مهم للتنبؤ)
from pyspark.sql.functions import lit
new_data = new_data.withColumn("label", lit(0))

# استخدمي نفس pipeline_model اللي دربتيه
new_data_transformed = pipeline_model.transform(new_data).select("features", "label")

predictions_new = rf_model.transform(new_data_transformed)

predictions_new.select("features", "prediction", "probability").show(truncate=False)

# =============================================
# ✅ 1. إنشاء بيانات جديدة بها CustomerID
# =============================================
new_data = spark.createDataFrame([
    ("Customer_1", 0, 5, 1, 100.0, 0.2, 0.1, 7, "US", "Android App", "Cash", "Buyer"),
    ("Customer_2", 0, 2, 0, 50.0, 0.0, 0.0, 3, "IT", "IOS App", "Credit Card", "Browser"),
    ("Customer_3", 0, 10, 3, 200.0, 0.5, 0.0, 12, "FR", "Web", "PayPal", "Loyal")
], ["CustomerID"] + numeric_cols + categorical_cols)

# =============================================
# ✅ 2. إضافة عمود label مؤقت (مش للتدريب)
# =============================================
from pyspark.sql.functions import lit
new_data = new_data.withColumn("label", lit(0))

# =============================================
# ✅ 3. تطبيق نفس pipeline_model المستخدم أثناء التدريب
# =============================================
new_data_transformed = pipeline_model.transform(new_data).select("CustomerID", "features", "label")

# =============================================
# ✅ 4. تطبيق الموديل RandomForest المدرب على البيانات الجديدة
# =============================================
predictions_new = rf_model.transform(new_data_transformed)

# =============================================
# ✅ 5. عرض النتيجة بوضوح (ID + Prediction + Probabilities)
# =============================================
predictions_new.select(
    "CustomerID",
    "prediction",
    "probability"
).show(truncate=False)

import os
from pyspark.sql import SparkSession

# تحديد المسارات
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-11-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.5.0-bin-hadoop3"
os.environ["PATH"] += os.pathsep + os.path.join(os.environ["SPARK_HOME"], "bin")

# إنشاء SparkSession متصل بـ Google Cloud Storage
spark = SparkSession.builder \
    .appName("ChurnPrediction") \
    .config("spark.jars", "/content/gcs-connector.jar") \
    .config("spark.hadoop.fs.gs.impl", "com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem") \
    .config("spark.hadoop.google.cloud.auth.service.account.enable", "true") \
    .config("spark.hadoop.google.cloud.auth.service.account.json.keyfile", "/content/key.json") \
    .getOrCreate()

# اسم الـ bucket اللي هتحفظ فيه الموديل
gcs_bucket = "gs://bigdata-ai-datalake"

# المجلد اللي هيتم الحفظ فيه
model_path = f"{gcs_bucket}/models/clickstream_churn_model"

rf_model.save(model_path)
print(f"✅ Model saved successfully to: {model_path}")

!gsutil ls gs://bigdata-ai-datalake/models/

# from pyspark.ml.classification import RandomForestClassificationModel

# rf_loaded = RandomForestClassificationModel.load(model_path)
# print("✅ Model loaded successfully from GCS!")